{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brain_tumor_segmentation_title"
   },
   "source": [
    "# MONAI-based 3D Brain Tumor Segmentation\n",
    "## Optimized for Google Colab with A100 GPUs\n",
    "\n",
    "This notebook implements a robust 3D brain tumor segmentation pipeline using MONAI framework, designed to handle common issues and work efficiently with BraTS data on Google Colab.\n",
    "\n",
    "### Features:\n",
    "- ✅ Fixed ROI size issues for smaller volumes\n",
    "- ✅ Resolved tensor dimension mismatches\n",
    "- ✅ Autocast compatibility fixes\n",
    "- ✅ Support for .rar/.zip archive extraction\n",
    "- ✅ Optimized for A100 GPU performance\n",
    "- ✅ Robust training/validation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's clone the repository and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/hrishikeshHD/BrainTumorSegmentation.git\n",
    "%cd BrainTumorSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_environment"
   },
   "outputs": [],
   "source": [
    "# Run the Colab setup script\n",
    "!python colab_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_check"
   },
   "source": [
    "## 2. GPU and Environment Check\n",
    "\n",
    "Let's verify that we have access to a GPU and check our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MONAI version: {monai.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Print MONAI configuration\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_preparation"
   },
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### Option A: Upload your own BraTS data\n",
    "\n",
    "If you have BraTS data in .zip or .rar format, upload it to `/content/data/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "\n",
    "# Upload files\n",
    "print(\"Please upload your BraTS data (.zip or .rar files):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to data directory\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'/content/data/{filename}')\n",
    "    print(f\"Uploaded: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "extract_archives"
   },
   "source": [
    "### Extract Archives (if needed)\n",
    "\n",
    "The segmentation script will automatically extract .zip and .rar files, but you can also do it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "manual_extract"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import rarfile\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('/content/data')\n",
    "\n",
    "# Extract ZIP files\n",
    "for zip_file in data_path.glob('*.zip'):\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "    print(f\"Extracted: {zip_file.name}\")\n",
    "\n",
    "# Extract RAR files\n",
    "for rar_file in data_path.glob('*.rar'):\n",
    "    with rarfile.RarFile(rar_file, 'r') as rar_ref:\n",
    "        rar_ref.extractall(data_path)\n",
    "    print(f\"Extracted: {rar_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_data_structure"
   },
   "source": [
    "### Verify Data Structure\n",
    "\n",
    "Let's check if the data is properly organized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data_structure"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def check_data_structure(base_path):\n",
    "    base = Path(base_path)\n",
    "    \n",
    "    processed_data = base / 'processed_data'\n",
    "    train_dir = processed_data / 'train'\n",
    "    test_dir = processed_data / 'test'\n",
    "    \n",
    "    print(f\"Checking data structure in: {base}\")\n",
    "    print(f\"Processed data exists: {processed_data.exists()}\")\n",
    "    print(f\"Train directory exists: {train_dir.exists()}\")\n",
    "    print(f\"Test directory exists: {test_dir.exists()}\")\n",
    "    \n",
    "    if train_dir.exists():\n",
    "        train_cases = list(train_dir.iterdir())\n",
    "        print(f\"Training cases found: {len(train_cases)}\")\n",
    "        if train_cases:\n",
    "            print(f\"Example case: {train_cases[0].name}\")\n",
    "    \n",
    "    if test_dir.exists():\n",
    "        test_cases = list(test_dir.iterdir())\n",
    "        print(f\"Test cases found: {len(test_cases)}\")\n",
    "        if test_cases:\n",
    "            print(f\"Example case: {test_cases[0].name}\")\n",
    "\n",
    "check_data_structure('/content/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "configuration"
   },
   "source": [
    "## 4. Configure Training Parameters\n",
    "\n",
    "Adjust these parameters based on your data and requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_params"
   },
   "outputs": [],
   "source": [
    "# Training configuration optimized for A100 GPU\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"max_epochs\": 100,\n",
    "    \"batch_size\": 2,  # Increase if you have more GPU memory\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"roi_size\": (128, 128, 128),  # Will adapt to smaller volumes automatically\n",
    "    \"sw_batch_size\": 4,\n",
    "    \"spacing\": (1.0, 1.0, 1.0),\n",
    "    \"num_classes\": 4,  # Background, NCR/NET, ED, ET\n",
    "    \"model_name\": \"unet\",  # Options: unet, segresnet, unetr\n",
    "    \"use_amp\": True,  # Automatic Mixed Precision for A100\n",
    "    \"cache_rate\": 0.5,\n",
    "    \"num_workers\": 4,\n",
    "    \"log_interval\": 50,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## 5. Start Training\n",
    "\n",
    "Now let's run the brain tumor segmentation training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# Import the segmentation pipeline\n",
    "from brain_tumor_segmentation import BrainTumorSegmentation, create_file_list\n",
    "import logging\n",
    "\n",
    "# Setup logging to see training progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize the segmentation pipeline\n",
    "segmentation = BrainTumorSegmentation(config)\n",
    "\n",
    "# Setup data directories (will handle archive extraction automatically)\n",
    "train_dir, test_dir = segmentation.setup_data_directories('/content/data')\n",
    "\n",
    "print(f\"Train directory: {train_dir}\")\n",
    "print(f\"Test directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_file_lists"
   },
   "outputs": [],
   "source": [
    "# Create file lists for training and validation\n",
    "train_files = create_file_list(train_dir, \"train\")\n",
    "val_files = create_file_list(test_dir, \"test\")\n",
    "\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(val_files)}\")\n",
    "\n",
    "if train_files:\n",
    "    print(\"\\nExample training file:\")\n",
    "    print(train_files[0])\n",
    "else:\n",
    "    print(\"\\n⚠️ No training files found! Please check your data structure.\")\n",
    "    print(\"Expected structure: processed_data/train/CaseName/CaseName_modality.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training (this will take a while)\n",
    "if train_files:\n",
    "    print(\"Starting training...\")\n",
    "    print(\"This may take several hours depending on your data size and epochs.\")\n",
    "    \n",
    "    # Start the training process\n",
    "    segmentation.train(train_files, val_files)\n",
    "    \n",
    "    print(\"\\n✅ Training completed!\")\n",
    "    print(\"Best model saved as: best_metric_model.pth\")\n",
    "else:\n",
    "    print(\"❌ Cannot start training without data. Please upload and organize your BraTS data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitoring_section"
   },
   "source": [
    "## 6. Monitor Training (Optional)\n",
    "\n",
    "You can monitor training progress and GPU usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitor_gpu"
   },
   "outputs": [],
   "source": [
    "# Monitor GPU usage during training\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_memory"
   },
   "outputs": [],
   "source": [
    "# Check memory usage\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"CPU Memory Usage: {psutil.virtual_memory().percent}%\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "## 7. Results and Model Evaluation\n",
    "\n",
    "After training, you can evaluate the model and visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_best_model"
   },
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "import torch\n",
    "from brain_tumor_segmentation import BrainTumorSegmentation\n",
    "\n",
    "# Load model state\n",
    "if os.path.exists('best_metric_model.pth'):\n",
    "    print(\"✅ Best model found!\")\n",
    "    \n",
    "    # Create model instance\n",
    "    segmentation = BrainTumorSegmentation(config)\n",
    "    model = segmentation.create_model()\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(torch.load('best_metric_model.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"❌ No trained model found. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_model"
   },
   "source": [
    "## 8. Download Trained Model\n",
    "\n",
    "Download the trained model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_trained_model"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Download the best model\n",
    "if os.path.exists('best_metric_model.pth'):\n",
    "    files.download('best_metric_model.pth')\n",
    "    print(\"Model downloaded successfully!\")\n",
    "else:\n",
    "    print(\"No model file found to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## 9. Troubleshooting\n",
    "\n",
    "Common issues and solutions:\n",
    "\n",
    "### Memory Issues\n",
    "- Reduce `batch_size` in config\n",
    "- Reduce `roi_size` to (96, 96, 96) or (64, 64, 64)\n",
    "- Set `cache_rate` to 0.1 or 0.2\n",
    "\n",
    "### Data Loading Issues\n",
    "- Check that your data follows BraTS naming convention\n",
    "- Ensure files are in .nii.gz format\n",
    "- Verify the folder structure matches: processed_data/{train|test}/\n",
    "\n",
    "### GPU Issues\n",
    "- Make sure you selected GPU runtime in Colab\n",
    "- Check GPU availability with `torch.cuda.is_available()`\n",
    "\n",
    "### Training Issues\n",
    "- Start with fewer epochs for testing\n",
    "- Monitor loss curves for convergence\n",
    "- Adjust learning rate if needed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}